{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GOSHEN  OLATUNBOSUN  \n",
      "ICAN  PROFESSIONAL  STAGE  \n",
      "  \n",
      "  \n",
      "   Ikorodu, Lagos •• Cell: +2348119592939  • goshenbosun002@gmail.com    \n",
      "Professional Summary  \n",
      "An accounting graduate seeking an opportunity in accounting and finance to utilize my skills in your industry, in \n",
      "addition, I’m highly motivated, detailed oriented, and knowledgeable in the use of the most common accounting \n",
      "software programs. Strong work ethic also commitment to integrity and accurate record keeping. Eager to join a team \n",
      "and help improve an organizati on’s financial focus.  \n",
      "Skills  \n",
      "● Solid working knowledge of MS office with a strong \n",
      "level of proficiency in Excel  \n",
      "● Excellent organizational skills with demonstrated \n",
      "ability to effectively prioritize to meet strict deadlines.  \n",
      "● Exceptional knowledge of accounting concepts as \n",
      "well as mathematics, accounting and finance topics.  ● Preparation of accurate financial accounts and reports \n",
      "to comply with accounting principles and practices.  \n",
      "● Proven ability to quickly learn and use new \n",
      "technologies  \n",
      "● Excellent written and communication skills.  \n",
      "● Excellent understanding of Digital marketing  \n",
      "Work History  \n",
      "Accounting Inter n, 2023 (NYSC)   \n",
      "TOAUSIB  – Abia State  \n",
      "● Identified and investigated financial fraud cases  \n",
      "● Analyzing financial statement  as well as fraudulent transactions  \n",
      "● Preparation of report on findings  \n",
      " \n",
      "Economics & commerce Teacher , 2022 - 2023  \n",
      "Great Brains Academy  – Abia State  \n",
      "● Presented lectures and conducted discussions to increase student knowledge  and competence  \n",
      "● Evaluated individual student learning and performance s and communicated necessary information to students  \n",
      "● Conducted extra -curricular activities for the students that improved all round learning   \n",
      "  \n",
      "Accounting Teacher , 2021 - 2022  \n",
      "Royal Diadem Group of School  – Lagos State  \n",
      "● Presented lectures and conducted discussions to increase student knowledge  and competence  \n",
      "● Evaluated individual student learning and performances and communicated necessary information to students  \n",
      "● I helped secure a Facebook grant of half a million naira(#500,000) and free ads  \n",
      "Social Media Manager , -Present   \n",
      "Scaling Ideas  – Lagos State  \n",
      "TOAUSIB - Abia State  \n",
      "Royal Diadem Group of Schools  – Lagos State  \n",
      "● Increased customer engagement through social media  \n",
      "● Provided digital marketing solutions that have helped these businesses grow  \n",
      "● I create co ntent and run ads regularly that have been yielding fruitful results  \n",
      "Education  \n",
      "NYSC Discharge Certificate          2023  \n",
      "Ekiti State University                       2021  \n",
      "Bsc Accounting  \n",
      "CGPA: 4.17/5.0  \n",
      "Omolaja Sodipo Memorial Anglican School  \n",
      "Senior Secondary  Certificate Examination (SSCE)       2016  \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"text.pdf\")\n",
    "pages = loader.load()\n",
    "page = pages[0]\n",
    "print(page.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zioneeel International College  \n",
      "Junior Secondary Certificate Examination        2013  \n",
      "Royal Diadem Demonstration School         2009  \n",
      " \n",
      "Certifications  \n",
      "Institute of Chartered Accountant of Nigeria (Professional Stage)  \n",
      "Digital Literacy (Microsoft)  \n",
      "Certified in digital marketing (Google)  \n",
      "Digital marketing booth camp (Digic lan Africa)  \n",
      "Advanced Social Media Management (Simplilearn)  \n",
      " \n",
      " \n",
      "REFEREES 0 \n",
      "Ilemobayo  A. Akingbomdere   [FCA]  \n",
      "CEO  \n",
      "IleAkin Financial Consulting  \n",
      "+23423181112  \n",
      " \n",
      "Mr. Olatunbosun Olaniyi  \n",
      "Director  \n",
      "Mutual Alliance, Lagos State  \n",
      "+2348062147426  \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "page2 = pages[1]\n",
    "print(page2.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'text.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "print(page2.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_1 = CharacterTextSplitter( separator= \"\", chunk_size = 150, chunk_overlap = 10 , length_function = len )\n",
    "\n",
    "docs = text_splitter_1.split_documents(pages)\n",
    "\n",
    "\n",
    "print(len(docs))\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convert the text/splits into embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#embedding1 = embeddings.embed_query(\"i like dogs\")\n",
    "#embedding2 = embeddings.embed_query(\"i like canines\")\n",
    "#enbedding3 = embeddings.embed_query(\"the weather is ugly outside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#print(np.dot(embedding1 , embedding2))\n",
    "#print(np.dot(embedding2, enbedding3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our vector embeddings into chromadb. ##chroma is light weight and in memory##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "#!rm -rf ./docs/chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#Actually creating the chromadb vector database.\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents= docs,\n",
    "    embedding= embeddings,\n",
    "    persist_directory= persist_directory\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "question = \"does he have good speaking and penning with online expertise?\"\n",
    "\n",
    "response = vectordb.similarity_search(question, k=3)\n",
    "\n",
    "print(len(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proven ability to quickly learn and use new \n",
      "technologies  \n",
      "● Excellent written and communication skills.  \n",
      "● Excellent understanding of Digital mark\n",
      "Proven ability to quickly learn and use new \n",
      "technologies  \n",
      "● Excellent written and communication skills.  \n",
      "● Excellent understanding of Digital mark\n",
      "● Solid working knowledge of MS office with a strong \n",
      "level of proficiency in Excel  \n",
      "● Excellent organizational skills with demonstrated \n",
      "ability t\n"
     ]
    }
   ],
   "source": [
    "print(response[0].page_content)\n",
    "print(response[1].page_content)\n",
    "print(response[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Using maximum marginal Relevanve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proven ability to quickly learn and use new \n",
      "technologies  \n",
      "● Excellent written and communication skills.  \n",
      "● Excellent understanding of Digital mark\n",
      "~~~\n",
      "GOSHEN  OLATUNBOSUN  \n",
      "ICAN  PROFESSIONAL  STAGE  \n",
      "  \n",
      "  \n",
      "   Ikorodu, Lagos •• Cell: +2348119592939  • goshenbosun002@gmail.com    \n",
      "Professional Summa\n",
      "~~~\n",
      "student learning and performances and communicated necessary information to students  \n",
      "● I helped secure a Facebook grant of half a million naira(#500\n"
     ]
    }
   ],
   "source": [
    "question2 = \"does he have good speaking and penning with online expertise?\"\n",
    "\n",
    "response2 = vectordb.max_marginal_relevance_search(question2, k=3)\n",
    "\n",
    "print(response2[0].page_content)\n",
    "print(\"~~~\")\n",
    "print(response2[1].page_content)\n",
    "print(\"~~~\")\n",
    "print(response2[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval using SelfQuery (i.e LLM aided retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appropriate imports \n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot import lark, please install it with 'pip install lark'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m document_content_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGoshen\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Resume (CV)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m llm \u001b[39m=\u001b[39m OpenAI(temperature\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m retriever \u001b[39m=\u001b[39m SelfQueryRetriever\u001b[39m.\u001b[39;49mfrom_llm(\n\u001b[0;32m     18\u001b[0m     llm, \n\u001b[0;32m     19\u001b[0m     vectordb,\n\u001b[0;32m     20\u001b[0m     document_content_description, \n\u001b[0;32m     21\u001b[0m     metadata_field_info, \n\u001b[0;32m     22\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\My_python_stuff\\LangChain\\.chatBot\\lib\\site-packages\\langchain\\retrievers\\self_query\\base.py:166\u001b[0m, in \u001b[0;36mSelfQueryRetriever.from_llm\u001b[1;34m(cls, llm, vectorstore, document_contents, metadata_field_info, structured_query_translator, chain_kwargs, enable_limit, use_original_query, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mallowed_operators\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m chain_kwargs:\n\u001b[0;32m    163\u001b[0m     chain_kwargs[\n\u001b[0;32m    164\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mallowed_operators\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m     ] \u001b[39m=\u001b[39m structured_query_translator\u001b[39m.\u001b[39mallowed_operators\n\u001b[1;32m--> 166\u001b[0m llm_chain \u001b[39m=\u001b[39m load_query_constructor_chain(\n\u001b[0;32m    167\u001b[0m     llm,\n\u001b[0;32m    168\u001b[0m     document_contents,\n\u001b[0;32m    169\u001b[0m     metadata_field_info,\n\u001b[0;32m    170\u001b[0m     enable_limit\u001b[39m=\u001b[39menable_limit,\n\u001b[0;32m    171\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mchain_kwargs,\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    174\u001b[0m     llm_chain\u001b[39m=\u001b[39mllm_chain,\n\u001b[0;32m    175\u001b[0m     vectorstore\u001b[39m=\u001b[39mvectorstore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    179\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\My_python_stuff\\LangChain\\.chatBot\\lib\\site-packages\\langchain\\chains\\query_constructor\\base.py:155\u001b[0m, in \u001b[0;36mload_query_constructor_chain\u001b[1;34m(llm, document_contents, attribute_info, examples, allowed_comparators, allowed_operators, enable_limit, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_query_constructor_chain\u001b[39m(\n\u001b[0;32m    130\u001b[0m     llm: BaseLanguageModel,\n\u001b[0;32m    131\u001b[0m     document_contents: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMChain:\n\u001b[0;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a query constructor chain.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m        A LLMChain that can be used to construct queries.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     prompt \u001b[39m=\u001b[39m _get_prompt(\n\u001b[0;32m    156\u001b[0m         document_contents,\n\u001b[0;32m    157\u001b[0m         attribute_info,\n\u001b[0;32m    158\u001b[0m         examples\u001b[39m=\u001b[39;49mexamples,\n\u001b[0;32m    159\u001b[0m         allowed_comparators\u001b[39m=\u001b[39;49mallowed_comparators,\n\u001b[0;32m    160\u001b[0m         allowed_operators\u001b[39m=\u001b[39;49mallowed_operators,\n\u001b[0;32m    161\u001b[0m         enable_limit\u001b[39m=\u001b[39;49menable_limit,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m LLMChain(llm\u001b[39m=\u001b[39mllm, prompt\u001b[39m=\u001b[39mprompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\My_python_stuff\\LangChain\\.chatBot\\lib\\site-packages\\langchain\\chains\\query_constructor\\base.py:116\u001b[0m, in \u001b[0;36m_get_prompt\u001b[1;34m(document_contents, attribute_info, examples, allowed_comparators, allowed_operators, enable_limit)\u001b[0m\n\u001b[0;32m    112\u001b[0m prefix \u001b[39m=\u001b[39m DEFAULT_PREFIX\u001b[39m.\u001b[39mformat(schema\u001b[39m=\u001b[39mschema)\n\u001b[0;32m    113\u001b[0m suffix \u001b[39m=\u001b[39m DEFAULT_SUFFIX\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m     i\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(examples) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, content\u001b[39m=\u001b[39mdocument_contents, attributes\u001b[39m=\u001b[39mattribute_str\n\u001b[0;32m    115\u001b[0m )\n\u001b[1;32m--> 116\u001b[0m output_parser \u001b[39m=\u001b[39m StructuredQueryOutputParser\u001b[39m.\u001b[39;49mfrom_components(\n\u001b[0;32m    117\u001b[0m     allowed_comparators\u001b[39m=\u001b[39;49mallowed_comparators, allowed_operators\u001b[39m=\u001b[39;49mallowed_operators\n\u001b[0;32m    118\u001b[0m )\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m FewShotPromptTemplate(\n\u001b[0;32m    120\u001b[0m     examples\u001b[39m=\u001b[39mexamples,\n\u001b[0;32m    121\u001b[0m     example_prompt\u001b[39m=\u001b[39mEXAMPLE_PROMPT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     output_parser\u001b[39m=\u001b[39moutput_parser,\n\u001b[0;32m    126\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\My_python_stuff\\LangChain\\.chatBot\\lib\\site-packages\\langchain\\chains\\query_constructor\\base.py:73\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.from_components\u001b[1;34m(cls, allowed_comparators, allowed_operators)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_components\u001b[39m(\n\u001b[0;32m     59\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m     60\u001b[0m     allowed_comparators: Optional[Sequence[Comparator]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     61\u001b[0m     allowed_operators: Optional[Sequence[Operator]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructuredQueryOutputParser:\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    Create a structured query output parser from components.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m        a structured query output parser\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     ast_parser \u001b[39m=\u001b[39m get_parser(\n\u001b[0;32m     74\u001b[0m         allowed_comparators\u001b[39m=\u001b[39;49mallowed_comparators, allowed_operators\u001b[39m=\u001b[39;49mallowed_operators\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(ast_parse\u001b[39m=\u001b[39mast_parser\u001b[39m.\u001b[39mparse)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\My_python_stuff\\LangChain\\.chatBot\\lib\\site-packages\\langchain\\chains\\query_constructor\\parser.py:150\u001b[0m, in \u001b[0;36mget_parser\u001b[1;34m(allowed_comparators, allowed_operators)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39m# QueryTransformer is None when Lark cannot be imported.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m QueryTransformer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot import lark, please install it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpip install lark\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m transformer \u001b[39m=\u001b[39m QueryTransformer(\n\u001b[0;32m    154\u001b[0m     allowed_comparators\u001b[39m=\u001b[39mallowed_comparators, allowed_operators\u001b[39m=\u001b[39mallowed_operators\n\u001b[0;32m    155\u001b[0m )\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m Lark(GRAMMAR, parser\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlalr\u001b[39m\u001b[39m\"\u001b[39m, transformer\u001b[39m=\u001b[39mtransformer, start\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprogram\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Cannot import lark, please install it with 'pip install lark'."
     ]
    }
   ],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name = \"source\",\n",
    "        description = \"The origin on the chunk. should be 'text.pdf' \",\n",
    "        type = \"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"page\",\n",
    "        description = \"The page from which the chunk was taken. There are two pages in the document\",\n",
    "        type = \"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Goshen's Resume (CV)\"\n",
    "llm = OpenAI(temperature= 0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, \n",
    "    vectordb,\n",
    "    document_content_description, \n",
    "    metadata_field_info, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".chatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
